{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pLsI6Mpf76wm",
   "metadata": {
    "id": "pLsI6Mpf76wm"
   },
   "source": [
    "# HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6982465",
   "metadata": {
    "id": "f6982465"
   },
   "outputs": [],
   "source": [
    "# Name: Seyyid Osman Sevgili    \n",
    "# ID: 504221565"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d16488f",
   "metadata": {
    "id": "4d16488f"
   },
   "source": [
    "## PART I - Convolutional Neural Networks [12 pts]\n",
    "\n",
    "In this part of assignment, first we will implement convolution operation in 2 dimensions, then we will move to a Deep Learning framework for faster computation via GPUs!\n",
    "\n",
    "In this assignment, we will use the same API as in Assignment 1. You have implemented most of the required layers. You will add Conv2d layer under `DL/CNN.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a681fd",
   "metadata": {
    "id": "d5a681fd"
   },
   "outputs": [],
   "source": [
    "from DL.CNN import Conv2d\n",
    "from DL.checker.checks import *\n",
    "import numpy as np\n",
    "from DL.regularizers import Dropout, MaxPool2d, AveragePool2d, BatchNorm, BatchNorm2d\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d8a84cc",
   "metadata": {
    "id": "3d8a84cc"
   },
   "outputs": [],
   "source": [
    "# Additional imports\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc5e2d3",
   "metadata": {
    "id": "e3b381bf"
   },
   "source": [
    "### Convolutional Layer\n",
    "Implement and call the forward and backward passes for the convolutional layer in conv2d."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b381bf",
   "metadata": {
    "id": "e3b381bf"
   },
   "source": [
    "#### Forward Pass  [6 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac9704df",
   "metadata": {
    "id": "ac9704df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_forward_naive\n",
      "difference:  1.2496779136951377e-08\n"
     ]
    }
   ],
   "source": [
    "conv = Conv2d(in_size=1, out_size=1, kernel_size=4, stride=2, padding=1)\n",
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "conv.W = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "conv.b = np.linspace(-0.2, 0.3, num=3)\n",
    "\n",
    "\n",
    "out = conv.forward(x)\n",
    "# difference should be around 2e-8\n",
    "print('Testing conv_forward_naive')\n",
    "relError = rel_error(out, \"CNN_forward\")\n",
    "print(f'difference: ', relError)\n",
    "assert 2.9e-8 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1055d6e",
   "metadata": {
    "id": "d1055d6e"
   },
   "source": [
    "#### Backward Pass  [6 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ded79c5f",
   "metadata": {
    "id": "ded79c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error: 2.0005888887163505e-06\n",
      "dw error: 1.0721666659468113e-06\n",
      "db error: 7.710746659802168e-09\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(250)\n",
    "conv = Conv2d(in_size=1, out_size=2, stride=1, padding=1, kernel_size=3)\n",
    "\n",
    "x = np.random.randn(3, 1, 6, 6)\n",
    "conv.W = np.random.randn(2, 1, 3, 3)\n",
    "conv.b = np.random.randn(2,)\n",
    "dout = np.random.randn(3, 2, 6, 6)\n",
    "\n",
    "dx_num = grad_check(lambda _: conv.forward(x), x, dout)\n",
    "dw_num = grad_check(lambda _: conv.forward(x), conv.W, dout)\n",
    "db_num = grad_check(lambda _: conv.forward(x), conv.b, dout)\n",
    "\n",
    "out = conv.forward(x)\n",
    "dx, dw, db = conv.backward(dout)\n",
    "\n",
    "print(f'dx error: {rel_error(dx, dx_num)}')\n",
    "print(f'dw error: {rel_error(dw, dw_num)}')\n",
    "print(f'db error: {rel_error(db, db_num)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b3161",
   "metadata": {
    "id": "755b3161"
   },
   "source": [
    "## PART II - Regularizers and Pooling  [32 pts]\n",
    "\n",
    "You are going to implement regularization techniques widely used until recently in convolutional networks such as **Max Pooling** and **Dropout**\n",
    "\n",
    "Find `Dropout`, `MaxPool2d`, `AveragePool2d`, `BatchNorm` and `BatchNorm2d` classes in **`DL/regularizers.py`** and complete the implementation of `forward` and `backward` methods for both of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c018ab5",
   "metadata": {
    "id": "968c48e7"
   },
   "source": [
    "### Dropout layer\n",
    "\n",
    "As we covered in the class, dropout is a well-known regularization technique for preventing overfitting of neural networks. What dropout does is basically zeroing out of some outputs of hidden layers at random. We recommend you to multiply the dropout factor with outputs in forward pass as it is done in common implementations. Recall that this is called **Inverted Dropout**.\n",
    "\n",
    "For more information on dropout, you can check the paper below.\n",
    "\n",
    "**Improving neural networks by preventing co-adaptation of feature detectors**, Hinton et al.\n",
    "https://arxiv.org/pdf/1207.0580.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968c48e7",
   "metadata": {
    "id": "968c48e7"
   },
   "source": [
    "#### Forward pass  [3 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e13cab4",
   "metadata": {
    "id": "2e13cab4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout rate is: 0.3\n",
      "Percent of how much of input is zeroed out in training  0.29983, in testing 0.00000\n",
      "Dropout rate is: 0.5\n",
      "Percent of how much of input is zeroed out in training  0.50072, in testing 0.00000\n",
      "Dropout rate is: 0.8\n",
      "Percent of how much of input is zeroed out in training  0.80032, in testing 0.00000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(250)\n",
    "\n",
    "x = np.random.randn(500, 2000) + 250\n",
    "for p in [0.3, 0.5, 0.8]:\n",
    "    dropout = Dropout(p=p)\n",
    "    dropout.mode = 'train'\n",
    "    out = dropout.forward(x)\n",
    "    dropout.mode = 'test'\n",
    "    out_test = dropout.forward(x)\n",
    "\n",
    "    print(f'Dropout rate is: {p}')\n",
    "    print(f'Percent of how much of input is zeroed out in training  {(out == 0).mean():.5f}, in testing {(out_test == 0).mean():.5f}')\n",
    "\n",
    "# You can check wheter your implemention is true or not by looking at the percent of outputs set to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fa331f",
   "metadata": {
    "id": "30fa331f"
   },
   "source": [
    "#### Backward pass  [3 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f9001b6",
   "metadata": {
    "id": "8f9001b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on dx 1.892896245652478e-11\n"
     ]
    }
   ],
   "source": [
    "dropout = Dropout(p=0.75)\n",
    "np.random.seed(250)\n",
    "x = np.random.randn(12, 12) + 11\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "\n",
    "out = dropout.forward(x,seed=250)\n",
    "dx = dropout.backward(dout)\n",
    "dx_num = grad_check(lambda xx: dropout.forward(xx, seed=250), x, dout)\n",
    "\n",
    "relError = rel_error(dx, dx_num)\n",
    "print(f'Error on dx {relError}')\n",
    "assert 5e-10 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83addef7",
   "metadata": {
    "id": "83addef7"
   },
   "source": [
    "### MaxPool\n",
    "#### Forward Pass  [3 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8fffb38",
   "metadata": {
    "id": "e8fffb38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 5.000002532280781e-07\n"
     ]
    }
   ],
   "source": [
    "x_shape = (3, 3, 7, 7)\n",
    "x = np.linspace(-0.2, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "maxPool = MaxPool2d(stride = 2, pool_width = 3, pool_height = 3)\n",
    "out = maxPool.forward(x)\n",
    "\n",
    "relError = rel_error(out, \"maxpool_forward\")\n",
    "print(f'Error: {relError}')\n",
    "assert 1e-6 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa8a2a4",
   "metadata": {
    "id": "efa8a2a4"
   },
   "source": [
    "#### Backward pass  [3 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8acbbea",
   "metadata": {
    "id": "c8acbbea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_backward_naive function:\n",
      "dx error: 3.2756303484042576e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(250)\n",
    "x = np.random.randn(8, 1, 10, 10)\n",
    "dout = np.random.randn(8, 1, 5, 5)\n",
    "max_pool = MaxPool2d(pool_height=2, pool_width=2, stride=2)\n",
    "dx_num = grad_check(lambda x: max_pool.forward(x), x, dout)\n",
    "\n",
    "out = max_pool.forward(x)\n",
    "dx = max_pool.backward(dout)\n",
    "\n",
    "# Your error should be around 1e-12\n",
    "print('Testing max_pool_backward_naive function:')\n",
    "relError = rel_error(dx, dx_num)\n",
    "print(f'dx error: {relError}')\n",
    "assert 5e-12 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e611bf2",
   "metadata": {
    "id": "7e611bf2"
   },
   "source": [
    "### AveragePool\n",
    "#### Forward Pass  [3 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af188f7b",
   "metadata": {
    "id": "af188f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1.5999997413537592e-07\n"
     ]
    }
   ],
   "source": [
    "x_shape = (3, 3, 7, 7)\n",
    "x = np.linspace(-0.2, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "average_pool = AveragePool2d(stride = 2, pool_width = 3, pool_height = 3)\n",
    "out = average_pool.forward(x)\n",
    "\n",
    "relError = rel_error(out, \"averagepool_forward\")\n",
    "print(f'Error: {relError}')\n",
    "assert 2e-7 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69173012",
   "metadata": {
    "id": "69173012"
   },
   "source": [
    "#### Backward Pass  [3 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb97f345",
   "metadata": {
    "id": "bb97f345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing average_pool_backward_naive function:\n",
      "dx error: 2.548002714286369e-11\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(250)\n",
    "x = np.random.randn(8, 1, 10, 10)\n",
    "dout = np.random.randn(8, 1, 5, 5)\n",
    "average_pool = AveragePool2d(pool_height=2, pool_width=2, stride=2)\n",
    "dx_num = grad_check(lambda x: average_pool.forward(x), x, dout)\n",
    "\n",
    "out = average_pool.forward(x)\n",
    "dx = average_pool.backward(dout)\n",
    "\n",
    "# Your error should be around 1e-11\n",
    "print('Testing average_pool_backward_naive function:')\n",
    "relError = rel_error(dx, dx_num)\n",
    "print(f'dx error: {relError}')\n",
    "assert 5e-10 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f054d7",
   "metadata": {
    "id": "03f054d7"
   },
   "source": [
    "### Batch Normalization 1D\n",
    "\n",
    "#### Forward Pass  [3 pts]\n",
    "First read and understand the paper:\n",
    "\n",
    "S. Ioffe, C. Szegedy. 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n",
    "https://arxiv.org/pdf/1502.03167.pdf\n",
    "\n",
    "Implement the forward and backward passes for the Batch Normalization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61f5bd07",
   "metadata": {
    "id": "61f5bd07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without using batchnorm\n",
      "\t mean of each feature/channel: [3.26114746 2.70715814 2.91144663]\n",
      "\t stds of each feature/channel: [4.45385032 4.31325161 4.22674193]\n",
      "Stats after batch normalization with gamma=1, beta=0\n",
      "\t mean: [ 3.81176572e-16 -6.29126381e-17 -1.85037171e-17]\n",
      "\t std: [0.99999975 0.99999973 0.99999972]\n",
      "Stats after batch normalization with arbitirary parameters\n",
      "\t mean: [4. 2. 5.]\n",
      "\t std: [2.99999924 1.99999946 0.99999972]\n"
     ]
    }
   ],
   "source": [
    "# You should understand how the gamma and beta parameters affect to the output\n",
    "\n",
    "# An example of a single hidden layer with ReLU activation.\n",
    "np.random.seed(250)\n",
    "N, D1, D2 = 180, 60, 3,\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "a = np.maximum(0, X.dot(W1))\n",
    "\n",
    "bn1 = BatchNorm(D2)\n",
    "\n",
    "print('Without using batchnorm')\n",
    "print(f'\\t mean of each feature/channel: {a.mean(axis=0)}')\n",
    "print(f'\\t stds of each feature/channel: {a.std(axis=0)}')\n",
    "\n",
    "\n",
    "print('Stats after batch normalization with gamma=1, beta=0')\n",
    "normalized = bn1.forward(a)\n",
    "print(f'\\t mean: {normalized.mean(axis=0)}')\n",
    "print(f'\\t std: {normalized.std(axis=0)}')\n",
    "\n",
    "\n",
    "bn1.gamma = np.array([3.0, 2.0, 1.0])\n",
    "bn1.beta = np.array([4, 2, 5])\n",
    "normalized  = bn1.forward(a)\n",
    "print('Stats after batch normalization with arbitirary parameters')\n",
    "print(f'\\t mean: {normalized.mean(axis=0)}')\n",
    "print(f'\\t std: {normalized.std(axis=0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c249d01",
   "metadata": {
    "id": "1c249d01"
   },
   "source": [
    "#### Backward pass  [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dc2be41",
   "metadata": {
    "id": "1dc2be41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error: 1.0229882253791266e-08\n",
      "dgamma error: 5.47168175755361e-11\n",
      "dbeta error: 4.821210056265905e-12\n"
     ]
    }
   ],
   "source": [
    "# Gradient check batchnorm backward pass\n",
    "np.random.seed(250)\n",
    "N, D = 20, 6\n",
    "x = 3 * np.random.randn(N, D) + 9\n",
    "\n",
    "bn1 = BatchNorm(D)\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "fx = lambda x: bn1.forward(x, gamma=gamma, beta=beta)\n",
    "fg = lambda a: bn1.forward(x, gamma=a, beta=beta)\n",
    "fb = lambda b: bn1.forward(x, gamma=gamma, beta=b)\n",
    "\n",
    "dx_num = grad_check(fx, x, dout)\n",
    "da_num = grad_check(fg, gamma.copy(), dout)\n",
    "db_num = grad_check(fb, beta.copy(), dout)\n",
    "\n",
    "bn1.forward(x, gamma=gamma, beta=beta)\n",
    "dx, dgamma, dbeta = bn1.backward(dout)\n",
    "\n",
    "relError = rel_error(dx_num, dx)\n",
    "print(f'dx error: {relError}')\n",
    "assert 1e-7 > relError\n",
    "\n",
    "relError = rel_error(da_num, dgamma)\n",
    "print(f'dgamma error: {relError}')\n",
    "assert 1e-10 > relError\n",
    "\n",
    "relError = rel_error(db_num, dbeta)\n",
    "print(f'dbeta error: {relError}')\n",
    "assert 1e-11 > relError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TiYFrOyMtTfG",
   "metadata": {
    "id": "TiYFrOyMtTfG"
   },
   "source": [
    "### Batch Normalization 2D\n",
    "#### Forward Pass  [2 pts]\n",
    "\n",
    "Implement BatchNorm2d. This computes statistics per-channel over batch as in pytorch-Batchnorm2D. You can take the Pytorch documentation as reference.\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dtxa1K6GtT-m",
   "metadata": {
    "id": "dtxa1K6GtT-m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without using BatchNorm2d\n",
      "\t mean of each channel: [1.02624607 0.47773465 2.99564553]\n",
      "\t std of each channel: [1.9763066  2.95446124 1.00477191]\n",
      "\n",
      "Stats after BatchNorm2d normalization with gamma=1, beta=0\n",
      "\t mean: [3.60205692e-17 1.81583144e-17 4.04614613e-18]\n",
      "\t std: [0.99999488 0.99999484 0.99999505]\n",
      "\n",
      "Stats after BatchNorm2d normalization with arbitrary parameters\n",
      "\t mean: [4. 2. 5.]\n",
      "\t std: [2.99998464 1.99998969 0.99999505]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(250)\n",
    "N, C, H, W = 180, 3, 5, 5  # Batch size, channels, height, width\n",
    "\n",
    "# Generating random data with the correct shape\n",
    "X = np.random.randn(N, C, H, W)\n",
    "\n",
    "W = np.array([2., 3., 1.]).reshape((1,C,1,1))\n",
    "b = np.array([1., 0.5, 3.]).reshape((1,C,1,1))\n",
    "# Reshaping for the linear layer - simulating a linear transformation\n",
    "a = W*X + b\n",
    "\n",
    "# Creating a BatchNorm2d instance for the specified number of channels\n",
    "bn2d = BatchNorm2d(C)\n",
    "\n",
    "print('Without using BatchNorm2d')\n",
    "print(f'\\t mean of each channel: {a.mean(axis=(0,2,3))}')\n",
    "print(f'\\t std of each channel: {a.std(axis=(0,2,3))}')\n",
    "\n",
    "# Using BatchNorm2d with default parameters (gamma=1, beta=0)\n",
    "normalized = bn2d.forward(X)\n",
    "print('\\nStats after BatchNorm2d normalization with gamma=1, beta=0')\n",
    "print(f'\\t mean: {normalized.mean(axis=(0, 2, 3))}')\n",
    "print(f'\\t std: {normalized.std(axis=(0, 2, 3))}')\n",
    "\n",
    "# Changing gamma and beta parameters\n",
    "bn2d.gamma = np.array([3.0, 2.0, 1.0]).reshape((1,C,1,1))\n",
    "bn2d.beta = np.array([4, 2, 5]).reshape((1,C,1,1))\n",
    "normalized = bn2d.forward(X)\n",
    "\n",
    "print('\\nStats after BatchNorm2d normalization with arbitrary parameters')\n",
    "print(f'\\t mean: {normalized.mean(axis=(0, 2, 3))}')\n",
    "print(f'\\t std: {normalized.std(axis=(0, 2, 3))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K-BKhau-n31Q",
   "metadata": {
    "id": "K-BKhau-n31Q"
   },
   "source": [
    "#### Backward pass  [4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2Kr9Cvzln4ge",
   "metadata": {
    "id": "2Kr9Cvzln4ge"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error: 2.8076417360072243e-08\n",
      "dgamma error: 5.3621226844179e-12\n",
      "dbeta error: 3.275536604975915e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(250)\n",
    "N, C, H, W = 10, 3, 4, 4  # Batch size, channels, height, width\n",
    "\n",
    "x = 3 * np.random.randn(N, C, H, W) + 7\n",
    "\n",
    "bn2d = BatchNorm2d(C)\n",
    "gamma = np.random.randn(C).reshape((1,C,1,1))\n",
    "beta = np.random.randn(C).reshape((1,C,1,1))\n",
    "dout = np.random.randn(N, C, H, W)\n",
    "\n",
    "# Function to be used for numerical gradient calculation\n",
    "fx = lambda x: bn2d.forward(x, gamma=gamma, beta=beta)\n",
    "fg = lambda g: bn2d.forward(x, gamma=g, beta=beta)\n",
    "fb = lambda b: bn2d.forward(x, gamma=gamma, beta=b)\n",
    "\n",
    "# Gradient check for dx (input)\n",
    "dx_num = grad_check(fx, x, dout)\n",
    "\n",
    "# Gradient check for dgamma (gamma parameter)\n",
    "da_num = grad_check(fg, gamma.copy(), dout)\n",
    "\n",
    "# Gradient check for dbeta (beta parameter)\n",
    "db_num = grad_check(fb, beta.copy(), dout)\n",
    "\n",
    "# Perform the backward pass to get gradients from the BatchNorm2d layer\n",
    "bn2d.forward(x, gamma=gamma, beta=beta)\n",
    "dx, dgamma, dbeta = bn2d.backward(dout)\n",
    "\n",
    "# Calculate relative errors for each gradient\n",
    "rel_error_dx = rel_error(dx_num, dx)\n",
    "print(f'dx error: {rel_error_dx}')\n",
    "assert rel_error_dx < 1e-7\n",
    "\n",
    "rel_error_dgamma = rel_error(da_num, dgamma)\n",
    "print(f'dgamma error: {rel_error_dgamma}')\n",
    "assert rel_error_dgamma < 1e-10\n",
    "\n",
    "rel_error_dbeta = rel_error(db_num, dbeta)\n",
    "print(f'dbeta error: {rel_error_dbeta}')\n",
    "assert rel_error_dbeta < 1e-11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76450a54",
   "metadata": {
    "id": "76450a54"
   },
   "source": [
    "# PART III - Convolutional Neural Networks vs ResNets [28 pts]\n",
    "\n",
    "You can use Google Colab for the rest of the homework.\n",
    "\n",
    "## Environment setup\n",
    "Follow the tutorial about how to utilize Google Colab but **don't install PyTorch** as mentioned in the blog post.\n",
    "\n",
    "English:\n",
    "https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a845d1e5",
   "metadata": {
    "id": "a845d1e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "# This command should return some information about the GPU status if the runtime is right.\n",
    "# In addition to that, if you encounter memory issues, you can diagnose your model by this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00a1546b",
   "metadata": {
    "id": "00a1546b"
   },
   "outputs": [],
   "source": [
    "from DL import activations, layers, optimizers\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision import utils\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9t7ukBZcxyQH",
   "metadata": {
    "id": "9t7ukBZcxyQH"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e70e47a6",
   "metadata": {
    "id": "e70e47a6"
   },
   "outputs": [],
   "source": [
    "def fetch_dataloader():\n",
    "    # using random crops and horizontal flip for train set\n",
    "    train_transformer = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),  # randomly flip image horizontally\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "    # transformer for dev set\n",
    "    dev_transformer = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "\n",
    "    # ************************************************************************************\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data/data-cifar10', train=True,\n",
    "                                                download=True, transform=train_transformer)\n",
    "    devset = torchvision.datasets.CIFAR10(root='./data/data-cifar10', train=False,\n",
    "                                              download=True, transform=dev_transformer)\n",
    "\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                              shuffle=True, num_workers=0)\n",
    "\n",
    "    devloader = torch.utils.data.DataLoader(devset, batch_size=64,\n",
    "                                            shuffle=False, num_workers=0)\n",
    "    \n",
    "    return trainloader, devloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7be000",
   "metadata": {
    "id": "ec7be000"
   },
   "source": [
    "### Implement a Deep Neural Network with Convolutional Neural Network Architectures [4 pts]\n",
    "\n",
    "Use Convolutional Neural Network, Linear and Activation Layers to design a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ce4ae7d",
   "metadata": {
    "id": "6ce4ae7d"
   },
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implement a Convolutional Neural Network with (at least two) convolutional, linear and activation layers you have implemented\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Implement architecture\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # YOUR CODE STARTS\n",
    "        # self.conv1 = Conv2d(in_size=3, out_size=16, kernel_size=3, stride=1, padding=1)\n",
    "        # self.conv2 = Conv2d(in_size=16, out_size=32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # self.pool = MaxPool2d(stride = 2, pool_width = 3, pool_height = 3)\n",
    "\n",
    "        # self.fc1 = layers.AffineLayer(32 * 8 * 8, 128)\n",
    "        # self.fc2 = layers.AffineLayer(128, 0)\n",
    "\n",
    "        # self.relu = activations.ReLU()\n",
    "\n",
    "        # self.dropout = Dropout(p=0.5)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # YOUR CODE ENDS\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Implement forward-pass\n",
    "        \"\"\"\n",
    "\n",
    "         # YOUR CODE STARTS\n",
    "\n",
    "        # x = self.conv1.forward(x)\n",
    "        # x = self.relu.forward(x)\n",
    "        # x = self.pool.forward(x)\n",
    "\n",
    "        # x = self.conv2.forward(x)\n",
    "        # x = self.relu.forward(x)\n",
    "        # x = self.pool.forward(x)\n",
    "\n",
    "        # # Flatten\n",
    "        # x = x.view(x.size(0), -1)\n",
    "\n",
    "        # x = self.fc1.forward(x)\n",
    "        # x = self.relu.forward(x)\n",
    "\n",
    "        # x = self.dropout.forward(x)\n",
    "\n",
    "        # x = self.fc2.forward(x)\n",
    "\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # YOUR CODE ENDS\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3b0a9",
   "metadata": {
    "id": "ec7be000"
   },
   "source": [
    "### Implement a ResNet with residual blocks [4 pts]\n",
    "\n",
    "Residual Networks introduce skip connections to improve training dynamics. First, implement a simplified ResNet block. Then, use three residual blocks to build a small ResNet. Use layers you implemented above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2767f0a",
   "metadata": {
    "id": "6ce4ae7d"
   },
   "outputs": [],
   "source": [
    "class customResNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Implement architecture\n",
    "        \"\"\"\n",
    "        super(customResNet, self).__init__()\n",
    "\n",
    "        # self.conv1 = Conv2d(in_size=3, out_size=64, kernel_size=3, stride=1, padding=1)\n",
    "        # self.bn1 = BatchNorm2d(64)\n",
    "        # self.AvgPool2d = AveragePool2d(stride = 2, pool_width = 3, pool_height = 3)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        \n",
    "        self.AvgPool2d = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.layer1 = self.make_block(64, stride=1)\n",
    "        self.layer2 = self.make_block(128, stride=2)\n",
    "        self.layer3 = self.make_block(256, stride=2)\n",
    "        self.linear = nn.Linear(8192, 10)\n",
    "\n",
    "        \n",
    "    def make_block(self, ch=64, stride=2):\n",
    "    \n",
    "        # YOUR CODE STARTS\n",
    "        \n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(ch, ch*2, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch*2, ch*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch*2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "        # YOUR CODE ENDS    \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.AvgPool2d(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc77caa4",
   "metadata": {
    "id": "fc77caa4"
   },
   "source": [
    "### Initialize your networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f3c20d1",
   "metadata": {
    "id": "0f3c20d1"
   },
   "outputs": [],
   "source": [
    "networks = {\n",
    "            \"CNN\": CNN(), \n",
    "            \"customResNet\": customResNet()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51111359",
   "metadata": {
    "id": "51111359"
   },
   "source": [
    "### Initialization of the Optimizers: You can change the Parameters according to your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "489318db",
   "metadata": {
    "id": "489318db"
   },
   "outputs": [],
   "source": [
    "optimizers = {\n",
    "    \"CNN\": optim.Adam(networks[\"CNN\"].parameters(), lr=1e-3),\n",
    "    \"customResNet\": optim.Adam(networks[\"customResNet\"].parameters(), lr=1e-3),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab750e1",
   "metadata": {
    "id": "fab750e1"
   },
   "source": [
    "### Initialization of the losses: You can change the parameters according to your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21060791",
   "metadata": {
    "id": "21060791"
   },
   "outputs": [],
   "source": [
    "losses = {\n",
    "    \"CNN\": torch.nn.CrossEntropyLoss(reduction='sum'),\n",
    "    \"customResNet\": torch.nn.CrossEntropyLoss(reduction='sum'),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vey4sQDntKgN",
   "metadata": {
    "id": "Vey4sQDntKgN"
   },
   "source": [
    "### Setting the training length: You can change the parameters according to your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "oRtAeeoYtKgN",
   "metadata": {
    "id": "oRtAeeoYtKgN"
   },
   "outputs": [],
   "source": [
    "epochs = {\n",
    "    \"CNN\": 15,\n",
    "    \"customResNet\": 15\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32bfc7a",
   "metadata": {
    "id": "d32bfc7a"
   },
   "source": [
    "### Evaluation based on Accuracy [3 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94adec11",
   "metadata": {
    "id": "94adec11"
   },
   "outputs": [],
   "source": [
    "def evalf(network, test_loader, epoch, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Implement evaluation for accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE STARTS\n",
    "    \n",
    "    val_data = []\n",
    "    network.eval()  # Set the network to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = network(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    val_data = {'epoch': epoch + 1, 'loss': loss.item(), 'accuracy': accuracy}\n",
    "    # YOUR CODE ENDS\n",
    "\n",
    "    return accuracy, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6501cf0c",
   "metadata": {},
   "source": [
    "### Training Loop [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af56487d",
   "metadata": {
    "id": "af56487d"
   },
   "outputs": [],
   "source": [
    "def train(network, optimizer, loss_fn, train_loader, epochs=10, verbose=True, device=\"cpu\", test_loader=None):\n",
    "    \"\"\"\n",
    "    Implement training loop\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE STARTS\n",
    "    \n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    \n",
    "    network.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        network.train()\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = network(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Statistics\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "        epoch_accuracy = 100.0 * correct / total\n",
    "        epoch_avg_loss = epoch_loss / len(train_loader)\n",
    "        train_data.append({'epoch': epoch + 1, 'loss': epoch_avg_loss, 'accuracy': epoch_accuracy})\n",
    "        _, val_d = evalf(network, test_loader, epoch, device)\n",
    "        val_data.append(val_d)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}] - Loss: {epoch_avg_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "    # YOUR CODE ENDS\n",
    "    return network, train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238a9bdd",
   "metadata": {
    "id": "238a9bdd"
   },
   "source": [
    "### Combine everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f14355",
   "metadata": {},
   "source": [
    "***I changed the code here for the saving model and results as required. I also changed the train and evalf functions calling orders.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3c56101",
   "metadata": {
    "id": "e3c56101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/15] - Loss: 110.5550, Accuracy: 35.76%\n",
      "Epoch [2/15] - Loss: 94.0117, Accuracy: 46.44%\n",
      "Epoch [3/15] - Loss: 87.7086, Accuracy: 50.49%\n",
      "Epoch [4/15] - Loss: 83.2127, Accuracy: 53.52%\n",
      "Epoch [5/15] - Loss: 79.8743, Accuracy: 55.48%\n",
      "Epoch [6/15] - Loss: 78.0579, Accuracy: 56.55%\n",
      "Epoch [7/15] - Loss: 76.2726, Accuracy: 57.91%\n",
      "Epoch [8/15] - Loss: 75.3944, Accuracy: 58.26%\n",
      "Epoch [9/15] - Loss: 73.5568, Accuracy: 59.69%\n",
      "Epoch [10/15] - Loss: 72.5173, Accuracy: 60.26%\n",
      "Epoch [11/15] - Loss: 71.4929, Accuracy: 60.83%\n",
      "Epoch [12/15] - Loss: 70.9021, Accuracy: 61.16%\n",
      "Epoch [13/15] - Loss: 69.7537, Accuracy: 61.91%\n",
      "Epoch [14/15] - Loss: 69.8388, Accuracy: 61.95%\n",
      "Epoch [15/15] - Loss: 69.0996, Accuracy: 62.34%\n",
      "Model saved to ./outputs/CNN.pkl\n",
      "for CNN|\t Accuracy: 69.12000\n",
      "Epoch [1/15] - Loss: 99.9579, Accuracy: 43.39%\n",
      "Epoch [2/15] - Loss: 70.7094, Accuracy: 60.74%\n",
      "Epoch [3/15] - Loss: 57.2518, Accuracy: 68.35%\n",
      "Epoch [4/15] - Loss: 47.6878, Accuracy: 74.12%\n",
      "Epoch [5/15] - Loss: 40.9237, Accuracy: 77.78%\n",
      "Epoch [6/15] - Loss: 36.0004, Accuracy: 80.57%\n",
      "Epoch [7/15] - Loss: 32.0000, Accuracy: 82.70%\n",
      "Epoch [8/15] - Loss: 29.1709, Accuracy: 84.30%\n",
      "Epoch [9/15] - Loss: 26.4639, Accuracy: 85.72%\n",
      "Epoch [10/15] - Loss: 24.0118, Accuracy: 87.06%\n",
      "Epoch [11/15] - Loss: 22.1742, Accuracy: 88.15%\n",
      "Epoch [12/15] - Loss: 20.3513, Accuracy: 89.03%\n",
      "Epoch [13/15] - Loss: 18.9704, Accuracy: 89.70%\n",
      "Epoch [14/15] - Loss: 17.4667, Accuracy: 90.57%\n",
      "Epoch [15/15] - Loss: 16.0583, Accuracy: 91.25%\n",
      "Model saved to ./outputs/customResNet.pkl\n",
      "for customResNet|\t Accuracy: 88.48000\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model(model, filename):\n",
    "    filepath = f\"./outputs/{filename}.pkl\"\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_data, val_data = {}, {}\n",
    "train_loader, test_loader = fetch_dataloader()\n",
    "\n",
    "for arch in [\"CNN\", \"customResNet\"]:\n",
    "    network, optimizer, loss_fn = networks[arch], optimizers[arch], losses[arch]\n",
    "\n",
    "    network, train_data[arch], val_data[arch] = train(network, optimizer, loss_fn, train_loader, epochs=epochs[arch], verbose=True, device=device, test_loader=test_loader)\n",
    "    save_model(network, arch)\n",
    "    accuracy, _ = evalf(network, test_loader, 0, device=device)\n",
    "    \n",
    "    print(f\"for {arch}|\\t Accuracy: {accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa0d248d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>tensor(19.6491, device='cuda:0')</td>\n",
       "      <td>51.70</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>tensor(18.4886, device='cuda:0')</td>\n",
       "      <td>54.33</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>tensor(17.5151, device='cuda:0')</td>\n",
       "      <td>60.06</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>tensor(19.5718, device='cuda:0')</td>\n",
       "      <td>62.31</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>tensor(15.8826, device='cuda:0')</td>\n",
       "      <td>64.62</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>tensor(18.1280, device='cuda:0')</td>\n",
       "      <td>64.55</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>tensor(17.9750, device='cuda:0')</td>\n",
       "      <td>65.81</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>tensor(14.8187, device='cuda:0')</td>\n",
       "      <td>66.10</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>tensor(13.1775, device='cuda:0')</td>\n",
       "      <td>67.74</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>tensor(14.8780, device='cuda:0')</td>\n",
       "      <td>67.62</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>tensor(14.6266, device='cuda:0')</td>\n",
       "      <td>68.27</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>tensor(14.1122, device='cuda:0')</td>\n",
       "      <td>68.86</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>tensor(11.5522, device='cuda:0')</td>\n",
       "      <td>69.16</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>tensor(13.0859, device='cuda:0')</td>\n",
       "      <td>69.35</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>tensor(12.6507, device='cuda:0')</td>\n",
       "      <td>69.12</td>\n",
       "      <td>CNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>tensor(18.8341, device='cuda:0')</td>\n",
       "      <td>56.40</td>\n",
       "      <td>customResNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>tensor(10.6495, device='cuda:0')</td>\n",
       "      <td>67.27</td>\n",
       "      <td>customResNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>tensor(11.0741, device='cuda:0')</td>\n",
       "      <td>69.55</td>\n",
       "      <td>customResNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>tensor(9.4143, device='cuda:0')</td>\n",
       "      <td>76.82</td>\n",
       "      <td>customResNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5</td>\n",
       "      <td>tensor(10.2043, device='cuda:0')</td>\n",
       "      <td>79.10</td>\n",
       "      <td>customResNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>tensor(5.5575, device='cuda:0')</td>\n",
       "      <td>80.90</td>\n",
       "      <td>customResNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7</td>\n",
       "      <td>tensor(3.5231, device='cuda:0')</td>\n",
       "      <td>79.58</td>\n",
       "      <td>customResNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8</td>\n",
       "      <td>tensor(6.7377, device='cuda:0')</td>\n",
       "      <td>83.61</td>\n",
       "      <td>customResNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9</td>\n",
       "      <td>tensor(8.4375, device='cuda:0')</td>\n",
       "      <td>84.34</td>\n",
       "      <td>customResNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10</td>\n",
       "      <td>tensor(4.6992, device='cuda:0')</td>\n",
       "      <td>85.93</td>\n",
       "      <td>customResNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>11</td>\n",
       "      <td>tensor(6.5202, device='cuda:0')</td>\n",
       "      <td>86.85</td>\n",
       "      <td>customResNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12</td>\n",
       "      <td>tensor(3.3639, device='cuda:0')</td>\n",
       "      <td>87.22</td>\n",
       "      <td>customResNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13</td>\n",
       "      <td>tensor(0.9931, device='cuda:0')</td>\n",
       "      <td>87.79</td>\n",
       "      <td>customResNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>14</td>\n",
       "      <td>tensor(1.9592, device='cuda:0')</td>\n",
       "      <td>88.23</td>\n",
       "      <td>customResNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15</td>\n",
       "      <td>tensor(0.8350, device='cuda:0')</td>\n",
       "      <td>88.48</td>\n",
       "      <td>customResNet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch                              loss  accuracy         Model\n",
       "0       1  tensor(19.6491, device='cuda:0')     51.70           CNN\n",
       "1       2  tensor(18.4886, device='cuda:0')     54.33           CNN\n",
       "2       3  tensor(17.5151, device='cuda:0')     60.06           CNN\n",
       "3       4  tensor(19.5718, device='cuda:0')     62.31           CNN\n",
       "4       5  tensor(15.8826, device='cuda:0')     64.62           CNN\n",
       "5       6  tensor(18.1280, device='cuda:0')     64.55           CNN\n",
       "6       7  tensor(17.9750, device='cuda:0')     65.81           CNN\n",
       "7       8  tensor(14.8187, device='cuda:0')     66.10           CNN\n",
       "8       9  tensor(13.1775, device='cuda:0')     67.74           CNN\n",
       "9      10  tensor(14.8780, device='cuda:0')     67.62           CNN\n",
       "10     11  tensor(14.6266, device='cuda:0')     68.27           CNN\n",
       "11     12  tensor(14.1122, device='cuda:0')     68.86           CNN\n",
       "12     13  tensor(11.5522, device='cuda:0')     69.16           CNN\n",
       "13     14  tensor(13.0859, device='cuda:0')     69.35           CNN\n",
       "14     15  tensor(12.6507, device='cuda:0')     69.12           CNN\n",
       "15      1  tensor(18.8341, device='cuda:0')     56.40  customResNet\n",
       "16      2  tensor(10.6495, device='cuda:0')     67.27  customResNet\n",
       "17      3  tensor(11.0741, device='cuda:0')     69.55  customResNet\n",
       "18      4   tensor(9.4143, device='cuda:0')     76.82  customResNet\n",
       "19      5  tensor(10.2043, device='cuda:0')     79.10  customResNet\n",
       "20      6   tensor(5.5575, device='cuda:0')     80.90  customResNet\n",
       "21      7   tensor(3.5231, device='cuda:0')     79.58  customResNet\n",
       "22      8   tensor(6.7377, device='cuda:0')     83.61  customResNet\n",
       "23      9   tensor(8.4375, device='cuda:0')     84.34  customResNet\n",
       "24     10   tensor(4.6992, device='cuda:0')     85.93  customResNet\n",
       "25     11   tensor(6.5202, device='cuda:0')     86.85  customResNet\n",
       "26     12   tensor(3.3639, device='cuda:0')     87.22  customResNet\n",
       "27     13   tensor(0.9931, device='cuda:0')     87.79  customResNet\n",
       "28     14   tensor(1.9592, device='cuda:0')     88.23  customResNet\n",
       "29     15   tensor(0.8350, device='cuda:0')     88.48  customResNet"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def convert_to_dataframe(model_name, data):\n",
    "    df = pd.DataFrame(data)\n",
    "    df['Model'] = model_name\n",
    "    return df\n",
    "    \n",
    "def combine_dataframes(data_dict, save_name):\n",
    "    dataframes = [convert_to_dataframe(model_name, data) for model_name, data in data_dict.items()]\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    combined_df.to_csv(f\"./outputs/{save_name}_results.csv\")\n",
    "    return combined_df\n",
    "\n",
    "combine_dataframes(train_data, \"train\")\n",
    "combine_dataframes(val_data, \"validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86c678c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model to CPU\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model to CPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m         loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m accuracy, _ \u001b[38;5;241m=\u001b[39m evalf(loaded_model, test_loader, \u001b[38;5;241m0\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     12\u001b[0m accuracy\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/storage.py:371\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_from_bytes\u001b[39m(b):\n\u001b[0;32m--> 371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/serialization.py:1040\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/serialization.py:1268\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1266\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1267\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1268\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m deserialized_storage_keys \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1272\u001b[0m offset \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;28;01mif\u001b[39;00m f_should_read_directly \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/serialization.py:1205\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1201\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_torch_load_uninitialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1205\u001b[0m         wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1206\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1207\u001b[0m         _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1208\u001b[0m     deserialized_objects[root_key] \u001b[38;5;241m=\u001b[39m typed_storage\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/serialization.py:391\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 391\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/serialization.py:266\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 266\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    268\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/serialization.py:250\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    247\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    252\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    253\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    254\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    255\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Load models\n",
    "with open(\"./outputs/customResNet.pkl\", \"rb\") as file:\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Loading model to GPU\")\n",
    "        loaded_model = pickle.load(file)\n",
    "    else:\n",
    "        print(\"Loading model to CPU\")\n",
    "        loaded_model = pickle.load(file)\n",
    "        \n",
    "accuracy, _ = evalf(loaded_model, test_loader, 0, device=device)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d03e83e",
   "metadata": {},
   "source": [
    "### Visualize The First Two Convolution Layer Filter/Kernels of CNN Model and comment on the apperance of the filters.  [3 pts]\n",
    "Display the filters/kernels in the first and second convolutional layers. You can change below code if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adf22608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the first two conv layer outputs, filter an example image with first conv filters\n",
    "# Code here\n",
    "def visualize_filters(filters, num_filters=8):\n",
    "    \"\"\"\n",
    "    Visualizes the first few filters of a convolutional layer.\n",
    "    \"\"\"\n",
    "    filters = filters.detach().cpu()\n",
    "    fig, axes = plt.subplots(1, num_filters, figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i >= filters.size(0): break\n",
    "        ax.imshow(filters[i].squeeze(), cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(\"First Conv Layer Filters\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the filters of the first convolutional layer\n",
    "visualize_filters(loaded_model.conv1.weight, num_filters=8)\n",
    "\n",
    "# Code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e10ddd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64, 32, 32])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1_output.squeeze().detach().cpu().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51f1452b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pass the image through the first convolutional layer\n",
    "with torch.no_grad():\n",
    "    conv1_output = loaded_model.conv1(next(iter(test_loader))[0].to(\"cuda\"))\n",
    "\n",
    "def visualize_feature_maps(feature_maps, num_maps=8):\n",
    "    \"\"\"\n",
    "    Visualizes the feature maps of a convolutional layer's output.\n",
    "    \"\"\"\n",
    "    feature_maps = feature_maps.squeeze().detach().cpu()[0,:,:,:]  # Remove batch dimension\n",
    "    fig, axes = plt.subplots(1, num_maps, figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i >= feature_maps.size(0): break\n",
    "        ax.imshow(feature_maps[i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(\"Feature Maps\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the outputs of the first convolutional layer\n",
    "visualize_feature_maps(conv1_output, num_maps=8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962f1ab1",
   "metadata": {
    "id": "962f1ab1"
   },
   "source": [
    "## Compare the results [9 pts]\n",
    "\n",
    "In this part, you will compare the results of both methods. While answering the questions below, consider the following points:\n",
    "\n",
    "- What are the advantages and disadvantages of these models?\n",
    "- In what scenarios would you use a CNN versus a custom ResNet?\n",
    "- Is early stopping necessary during training? If so, why?\n",
    "    \n",
    "Also, feel free to use any library you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tLZhhnkA3K3M",
   "metadata": {
    "id": "tLZhhnkA3K3M"
   },
   "source": [
    "#### Plot loss and accuracy curves per training epoch for both train and test sets. Make sure you put legend and labels. Comment on the results. [3 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c260bf",
   "metadata": {
    "id": "99c260bf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "--s20NOl39CN",
   "metadata": {
    "id": "--s20NOl39CN"
   },
   "source": [
    "#### Visualize a few example test outputs with true and predicted labels (both models). Make comments [3 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mEC0W_4A38yZ",
   "metadata": {
    "id": "mEC0W_4A38yZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "qTGZkcf121_L",
   "metadata": {
    "id": "qTGZkcf121_L"
   },
   "source": [
    "#### Compare number of parameters of the models. Comment your insights [3 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21574b7",
   "metadata": {
    "id": "a21574b7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b913af59",
   "metadata": {},
   "source": [
    "# PART IV Object Detection [28 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1518a49",
   "metadata": {},
   "source": [
    "Object detection is a computer vision task where the goal is to identify and localize objects within an image. \n",
    "Unlike image classification, where the focus is on assigning a single label to an image, object detection involves:\n",
    "- Predicting **what** objects are present (class labels).\n",
    "- Predicting **where** the objects are located (bounding boxes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6a4952",
   "metadata": {},
   "source": [
    "#### Image Classification vs Object Detection:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a400820b",
   "metadata": {},
   "source": [
    "Image classification answers the question: **What is in the image?**\n",
    "\n",
    "Object detection answers the questions: **What objects are in the image, and where are they located?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fc08b6",
   "metadata": {},
   "source": [
    "#### Why Object Detection?\n",
    "Object detection is used in a variety of real-world applications, such as:\n",
    "- **Autonomous vehicles**: Detecting pedestrians, vehicles, and traffic signs.\n",
    "- **Medical imaging**: Identifying abnormalities in scans (e.g., tumors).\n",
    "- **Surveillance**: Recognizing and tracking people or objects in video feeds.\n",
    "- **Retail**: Counting products or monitoring shelves.\n",
    "\n",
    "\n",
    "#### Components of Object Detection:\n",
    "\n",
    "A basic object detection network typically consists of two outputs:\n",
    "1. **Class Prediction**: A probability distribution over possible object classes.\n",
    "2. **Bounding Box Regression**: Coordinates of the bounding box enclosing the object.\n",
    "\n",
    "\n",
    "- The bounding box is represented as `[x_min, y_min, x_max, y_max]`, where:\n",
    "  - `(x_min, y_min)` is the top-left corner.\n",
    "  - `(x_max, y_max)` is the bottom-right corner.\n",
    "- The network predicts both **what** (class label) and **where** (bounding box coordinates)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816b3a39",
   "metadata": {},
   "source": [
    "### Define a Basic Object Detection Network\n",
    "The network should:\n",
    "- Take an image as input.\n",
    "- Output a class label and four bounding box coordinates.\n",
    "\n",
    "**Hints**:\n",
    "- The final layer should output both the class probabilities and bounding box coordinates.\n",
    "- Use the CNN you trained in the previous part as the backbone of your object detector. Add two heads to your backbone model, where one of the heads will predict the object class and the other will predict the object location. \n",
    "\n",
    "\n",
    "**Steps**:\n",
    "- Implement a network that consists of a backbone and two heads.\n",
    "- Build forward and backward propagation for the network.\n",
    "- Use separate loss functions for classification (cross-entropy) and bounding box regression (mean squared error).\n",
    "- Train the network on the object detection dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e356e8",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce403cdc",
   "metadata": {},
   "source": [
    "You will detect license plates:\n",
    "https://ieee-dataport.org/open-access/cd-lp-compressed-domain-license-plate-detection-database\n",
    "\n",
    "The dataset contains 2,400 vehicle images for license plate detection purposes. There are 3 subsets, where you'll use only pixel domain images (2400 images). Since you'll predict the license plate coordinates, number of classes will be 2 (plate and non-plate objects). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd58653d",
   "metadata": {},
   "source": [
    "#### Implement the network [10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5253898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0817bb8a",
   "metadata": {},
   "source": [
    "#### Train the network [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e1d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8edee9d",
   "metadata": {},
   "source": [
    "#### Evaluate the model and plot precision, recall, and mAP scores for different IoU thresholds [5 pts]\n",
    "\n",
    "Evaluate the network using **evaluate()** below for different IoU thresholds [0.1; 0.1; 1]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041b4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_iou(pred_box, gt_box):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU) between two bounding boxes.\n",
    "    Args:\n",
    "        pred_box (list): [x_min, y_min, x_max, y_max] for the predicted box.\n",
    "        gt_box (list): [x_min, y_min, x_max, y_max] for the ground truth box.\n",
    "    Returns:\n",
    "        float: IoU value.\n",
    "    \"\"\"\n",
    "    # Compute intersection\n",
    "    x_min_inter = max(pred_box[0], gt_box[0])\n",
    "    y_min_inter = max(pred_box[1], gt_box[1])\n",
    "    x_max_inter = min(pred_box[2], gt_box[2])\n",
    "    y_max_inter = min(pred_box[3], gt_box[3])\n",
    "\n",
    "    inter_area = max(0, x_max_inter - x_min_inter) * max(0, y_max_inter - y_min_inter)\n",
    "    \n",
    "    # Compute union\n",
    "    pred_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n",
    "    gt_area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n",
    "    union_area = pred_area + gt_area - inter_area\n",
    "\n",
    "    return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "def evaluate(predictions, ground_truths, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate object detection metrics: Precision, Recall, and mAP.\n",
    "    Args:\n",
    "        predictions (list): List of predicted bounding boxes [[x_min, y_min, x_max, y_max, class], ...].\n",
    "        ground_truths (list): List of ground truth boxes [[x_min, y_min, x_max, y_max, class], ...].\n",
    "        iou_threshold (float): IoU threshold to consider a prediction correct.\n",
    "    Returns:\n",
    "        dict: Precision, Recall, and mAP scores.\n",
    "    \"\"\"\n",
    "    tp = 0  # True positives\n",
    "    fp = 0  # False positives\n",
    "    fn = 0  # False negatives\n",
    "\n",
    "    matched_gt = set()  # Keep track of matched ground truths\n",
    "\n",
    "    for pred in predictions:\n",
    "        pred_box, pred_class = pred[:4], pred[4]\n",
    "        max_iou = 0\n",
    "        matched = None\n",
    "\n",
    "        for i, gt in enumerate(ground_truths):\n",
    "            gt_box, gt_class = gt[:4], gt[4]\n",
    "\n",
    "            # Only consider matching predictions of the same class\n",
    "            if pred_class == gt_class:\n",
    "                iou = calculate_iou(pred_box, gt_box)\n",
    "                if iou > max_iou and iou >= iou_threshold and i not in matched_gt:\n",
    "                    max_iou = iou\n",
    "                    matched = i\n",
    "\n",
    "        if matched is not None:\n",
    "            tp += 1\n",
    "            matched_gt.add(matched)\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "    fn = len(ground_truths) - len(matched_gt)\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "\n",
    "    # For simplicity, mAP can be computed as Precision in this example\n",
    "    # In real scenarios, mAP requires averaging precision at multiple recall thresholds\n",
    "    mAP = precision\n",
    "\n",
    "    return {\"Precision\": precision, \"Recall\": recall, \"mAP\": mAP}\n",
    "\n",
    "# Example data\n",
    "predictions = [\n",
    "    [50, 50, 150, 150, \"car\"],  # [x_min, y_min, x_max, y_max, class]\n",
    "    [30, 30, 120, 120, \"car\"],\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    [40, 40, 140, 140, \"car\"],  # [x_min, y_min, x_max, y_max, class]\n",
    "    [60, 60, 170, 170, \"car\"],\n",
    "]\n",
    "\n",
    "# Evaluate\n",
    "results = evaluate(predictions, ground_truths, iou_threshold=0.5)\n",
    "print(\"Evaluation Results:\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15a7b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0bed28",
   "metadata": {},
   "source": [
    "### Comparison [8 pts]\n",
    "Try different backbones: the Resnet you trained and an untrained (only initialized) CNN.\n",
    "Do backbones with different architectures affect the object detection performance?\n",
    "\n",
    "Compare the pretrained and untrained backbones: Do pretrained backbones ease object detection, in your experiments? If not, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c739ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare and comment here "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "de89bc0403a3685ad3f83983da38b1b3dd7896aeb9dd382d00c94cb25b5a82ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
